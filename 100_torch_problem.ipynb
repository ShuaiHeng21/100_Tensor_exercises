{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [5],\n",
      "        [6]])\n",
      "tensor([-1, -1, -1,  0,  0, -1, -1,  0])\n",
      "tensor([-1, -1, -1,  1,  1, -1, -1,  1])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([1,2,4,0,0,1,3,0])\n",
    "print(t.nonzero())\n",
    "t[t.nonzero().squeeze()] = -1\n",
    "print(t)\n",
    "mask = t==0\n",
    "t[~t.bool()] = 1 # equal to t[mask] = 1\n",
    "print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3, 4, 5, 9, 's'}\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a = set([1,2,3,5,1,1,4,9,'s'])\n",
    "print(a)\n",
    "print('s' in a)\n",
    "print(8 in a)\n",
    "print(2 in a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "torch.Size([4, 4])\n",
      "tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones([2,2])\n",
    "a = torch.nn.functional.pad(t,(1,1,1,1), mode=\"constant\", value=0)\n",
    "print(a)\n",
    "print(a.size())\n",
    "print(a.view(16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 0.],\n",
      "        [0., 0., 0., 3., 0.],\n",
      "        [0., 0., 0., 0., 4.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[ 0, -1,  0, -1,  0, -1,  0, -1],\n",
      "        [ 1,  0,  1,  0,  1,  0,  1,  0],\n",
      "        [ 0, -1,  0, -1,  0, -1,  0, -1],\n",
      "        [ 1,  0,  1,  0,  1,  0,  1,  0],\n",
      "        [ 0, -1,  0, -1,  0, -1,  0, -1],\n",
      "        [ 1,  0,  1,  0,  1,  0,  1,  0],\n",
      "        [ 0, -1,  0, -1,  0, -1,  0, -1],\n",
      "        [ 1,  0,  1,  0,  1,  0,  1,  0]])\n"
     ]
    }
   ],
   "source": [
    "below_diagonal = torch.Tensor([1,2,3,4])\n",
    "print(torch.diag(below_diagonal, diagonal=1))\n",
    "t = torch.zeros((8,8), dtype=int)\n",
    "t[1::2,::2] =1\n",
    "t[::2, 1::2] = -1\n",
    "print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0., 1., 0., 1.],\n",
      "        [1., 0., 1., 0., 1., 0.],\n",
      "        [0., 1., 0., 1., 0., 1.],\n",
      "        [1., 0., 1., 0., 1., 0.],\n",
      "        [0., 1., 0., 1., 0., 1.],\n",
      "        [1., 0., 1., 0., 1., 0.],\n",
      "        [0., 1., 0., 1., 0., 1.],\n",
      "        [1., 0., 1., 0., 1., 0.]])\n",
      "tensor([[-1.3605, -0.4815,  0.0240, -0.1746, -0.3165],\n",
      "        [ 0.3797, -0.2839, -0.3451,  0.7763, -1.0035],\n",
      "        [ 0.6386,  0.4955,  1.7062, -1.4454,  1.1355],\n",
      "        [ 1.5848, -0.8069,  0.0640,  1.3672, -1.4733],\n",
      "        [-0.0807,  1.7247, -1.1794, -1.1297,  0.1846]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t= torch.Tensor([[0,1],[1,0]])\n",
    "print(t.repeat([4,3]))\n",
    "t = torch.rand([5,5])\n",
    "print((t-t.mean())/t.std())\n",
    "5 //4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29. 将float类型的Tensor中元素进位转换为整数\n",
    "round away from zero, i.e. 3.2 => 4, -3.2 => -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.6550, -0.3341,  0.4113],\n",
      "        [-4.5893,  3.9331,  2.4995]])\n",
      "tensor([5.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3., -1.,  1.],\n",
       "        [-5.,  4.,  3.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn([2,3]) * 3\n",
    "print(t)\n",
    "print(torch.floor(torch.tensor([5.9])))\n",
    "torch.mul(torch.sign(t), torch.ceil(torch.abs(t)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30. 找出两个一维Tensor都出现的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "Z1 = np.random.randint(0,10,10)\n",
    "Z2 = np.random.randint(0,10,10)\n",
    "print(np.intersect1d(Z1, Z2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31. warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2016-06-01', '2016-06-02', '2016-06-03', '2016-06-04',\n",
       "       '2016-06-05', '2016-06-06', '2016-06-07', '2016-06-08',\n",
       "       '2016-06-09', '2016-06-10', '2016-06-11', '2016-06-12',\n",
       "       '2016-06-13', '2016-06-14', '2016-06-15', '2016-06-16',\n",
       "       '2016-06-17', '2016-06-18', '2016-06-19', '2016-06-20',\n",
       "       '2016-06-21', '2016-06-22', '2016-06-23', '2016-06-24',\n",
       "       '2016-06-25', '2016-06-26', '2016-06-27', '2016-06-28',\n",
       "       '2016-06-29', '2016-06-30', '2016-07-01', '2016-07-02',\n",
       "       '2016-07-03', '2016-07-04', '2016-07-05', '2016-07-06',\n",
       "       '2016-07-07', '2016-07-08', '2016-07-09', '2016-07-10',\n",
       "       '2016-07-11', '2016-07-12', '2016-07-13', '2016-07-14',\n",
       "       '2016-07-15', '2016-07-16', '2016-07-17', '2016-07-18',\n",
       "       '2016-07-19', '2016-07-20', '2016-07-21', '2016-07-22',\n",
       "       '2016-07-23', '2016-07-24', '2016-07-25', '2016-07-26',\n",
       "       '2016-07-27', '2016-07-28', '2016-07-29', '2016-07-30',\n",
       "       '2016-07-31'], dtype='datetime64[D]')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(-1)\n",
    "np.emath.sqrt(-1) == np.sqrt(-1)\n",
    "yesterday = np.datetime64('today','D') - np.timedelta64(1, 'D')\n",
    "yesterday\n",
    "today = np.datetime64('today','D')\n",
    "tomorrow = np.datetime64('today','D') + np.timedelta64(1, 'D')\n",
    "tomorrow\n",
    "Z = np.arange('2016-06','2016-08',dtype='datetime64[D]')\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "35. 在不复制值的情况下做原地计算（（A+B）*-(A/2)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5000, -1.5000, -1.5000])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.rand([3,3])\n",
    "B = torch.rand([3,3])\n",
    "B = A+B\n",
    "B*(-A/2)\n",
    "\n",
    "A = torch.ones(3)*1\n",
    "B = torch.ones(3) * 2\n",
    "torch.add(A,B, out=B)\n",
    "torch.div(A,2, out=A)\n",
    "torch.neg_(A)\n",
    "torch.mul(A, B, out=B)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "36. 使用5中不同方法提取Tensor中每个元素的整数部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6825, 4.1340],\n",
      "        [4.2248, 3.7058]])\n",
      "1: tensor([[0, 4],\n",
      "        [4, 3]], dtype=torch.int32)\n",
      "2: tensor([[0., 4.],\n",
      "        [4., 3.]])\n",
      "3: tensor([[0., 4.],\n",
      "        [4., 3.]])\n",
      "4: tensor([[0., 4.],\n",
      "        [4., 3.]])\n",
      "5: tensor([[0., 4.],\n",
      "        [4., 3.]])\n",
      "tensor([[0., 1., 2., 3., 4.],\n",
      "        [0., 1., 2., 3., 4.],\n",
      "        [0., 1., 2., 3., 4.],\n",
      "        [0., 1., 2., 3., 4.],\n",
      "        [0., 1., 2., 3., 4.]])\n",
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "Z = torch.rand([2,2])*5\n",
    "print(Z)\n",
    "# 1\n",
    "Z1 = Z.int()\n",
    "# Z1 = Z.astype(int)\n",
    "Z1 = Z.type(torch.int)\n",
    "print(\"1:\", Z1)\n",
    "Z2 = torch.floor(Z)\n",
    "print('2:', Z2)\n",
    "Z3 = Z - Z%1\n",
    "print(\"3:\", Z3)\n",
    "Z4 = torch.ceil(Z)-1\n",
    "print(\"4:\", Z4)\n",
    "print(\"5:\", torch.trunc(Z))\n",
    "a = torch.arange(5)\n",
    "# print(a)\n",
    "b = torch.ones([5,5])\n",
    "b[:] = a\n",
    "print(b)\n",
    "print(torch.arange(5).repeat(5,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "38. 创建一个生成器函数，用于生成10个整数，并用这个生成器函数创建一个一维Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8378, 0.2649, 0.2112, 0.3573, 0.2090, 0.0693, 0.8677, 0.6200, 0.1590,\n",
      "        0.0067])\n",
      "torch.return_types.sort(\n",
      "values=tensor([0.0067, 0.0693, 0.1590, 0.2090, 0.2112, 0.2649, 0.3573, 0.6200, 0.8378,\n",
      "        0.8677]),\n",
      "indices=tensor([9, 5, 8, 4, 2, 1, 3, 7, 0, 6]))\n",
      "tensor([0.8378, 0.2649, 0.2112, 0.3573, 0.2090, 0.0693, 0.8677, 0.6200, 0.1590,\n",
      "        0.0067])\n",
      "[tensor(0.0067), tensor(0.0693), tensor(0.1590), tensor(0.2090), tensor(0.2112), tensor(0.2649), tensor(0.3573), tensor(0.6200), tensor(0.8378), tensor(0.8677)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([0.0067, 0.0693, 0.1590, 0.2090, 0.2112, 0.2649, 0.3573, 0.6200, 0.8378,\n",
       "        0.8677]),\n",
       "indices=tensor([9, 5, 8, 4, 2, 1, 3, 7, 0, 6]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = (i for i in range(10))\n",
    "tensor = torch.tensor(list(generator))\n",
    "# tensor\n",
    "\n",
    "def g():\n",
    "    yield from range(10)\n",
    "torch.from_numpy(np.fromiter(g(), dtype=np.int))\n",
    "torch.linspace(0,1,12)[1:-1]\n",
    "torch.linspace(0,1,12)[1:-1]\n",
    "a = torch.rand(10)\n",
    "print(a)\n",
    "print(a.sort())\n",
    "a.sort()\n",
    "print(a)\n",
    "print(sorted(a))\n",
    "torch.sort(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "41. 对于小向量求和时，如何计算才能比使用np.sum更快？\n",
    "注：在numpy中，可以使用np.add.reduce, 在小向量计算速度上快于np.sum。但pyTorch似乎没有对应的方法，经测试，对torch.arange(10)进行求和时，torch.sum仍然快于np.add.reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00019669532775878906\n",
      "0.0012729167938232422\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "Z = np.arange(10)\n",
    "np.add.reduce(Z)\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)\n",
    "start_time = time.time()\n",
    "Z = torch.arange(10)\n",
    "torch.sum(Z)\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "41.给定两个随机TensorA和B，判断是否相等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(True)\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand([3,4])\n",
    "B = torch.rand([3,4])\n",
    "C = A\n",
    "print((A == B).sum() == A.numel())\n",
    "print((A == C).sum() == A.numel())\n",
    "# 检查值是否精确相等\n",
    "print(torch.equal(torch.Tensor([1.1, 2.1]), torch.Tensor([1,2])))\n",
    "# torch.Tensor是一个类，而torch.tensor是一个函数\n",
    "# 更经常使用torch.tensor，因为它更方便，有自动的类型转换\n",
    "# 而torch.Tensor确定为torch.float类型\n",
    "print(torch.equal(torch.tensor([1.1, 2.1]), torch.tensor([1, 2], dtype=torch.float)))\n",
    "print(torch.allclose(torch.tensor([1.1,2.1]), torch.tensor([1,2], dtype=torch.float), atol=0.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "43. 将Tensor转变为只读"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9.2158,  9.1773, 20.4982, 17.9178,  5.7871, 12.6672,  1.7377,  7.6244,\n",
      "         3.2433, 15.3436])\n",
      "tensor([[3.0358e+00, 9.5385e-01],\n",
      "        [3.0294e+00, 8.5647e-01],\n",
      "        [4.5275e+00, 1.0795e+00],\n",
      "        [4.2329e+00, 5.8689e-01],\n",
      "        [2.4056e+00, 3.9518e-03],\n",
      "        [3.5591e+00, 1.1017e+00],\n",
      "        [1.3182e+00, 6.9187e-01],\n",
      "        [2.7612e+00, 1.0164e-01],\n",
      "        [1.8009e+00, 4.7685e-02],\n",
      "        [3.9171e+00, 1.1480e+00]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3.0358e+00, 9.5385e-01],\n",
       "        [3.0294e+00, 8.5647e-01],\n",
       "        [4.5275e+00, 1.0795e+00],\n",
       "        [4.2329e+00, 5.8689e-01],\n",
       "        [2.4056e+00, 3.9514e-03],\n",
       "        [3.5591e+00, 1.1017e+00],\n",
       "        [1.3182e+00, 6.9187e-01],\n",
       "        [2.7612e+00, 1.0164e-01],\n",
       "        [1.8009e+00, 4.7685e-02],\n",
       "        [3.9171e+00, 1.1480e+00]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = np.zeros(10)\n",
    "Z.flags.writeable = False\n",
    "# Z[0] = 2 # read only -> ValueError\n",
    "A = torch.rand([10,2])*5\n",
    "# print(A)\n",
    "B = torch.zeros([10,2])\n",
    "print(A.square().sum(dim=1))\n",
    "B[:,0] = torch.sqrt(A.square().sum(dim=1))\n",
    "B[:,1] = torch.arccos(A[:,0]/ B[:,0])\n",
    "print(B)\n",
    "X = A[:,0]\n",
    "Y = A[:,1]\n",
    "R = torch.sqrt(X**2 + Y**2)\n",
    "T = torch.atan(Y/X)\n",
    "torch.stack([R,T],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4620, 0.5805, 0.1659, 0.2199, 0.0962, 0.3451, 0.5704, 0.6778, 0.5864,\n",
      "        0.6311])\n",
      "tensor([0.4620, 0.5805, 0.1659, 0.2199, 0.0962, 0.3451, 0.5704, 0.0000, 0.5864,\n",
      "        0.6311])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 1.0000],\n",
       "        [0.2500, 0.2500, 1.0000],\n",
       "        [0.5000, 0.5000, 1.0000],\n",
       "        [0.7500, 0.7500, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand(10)\n",
    "print(t)\n",
    "t[torch.argmax(t)] = 0\n",
    "print(t)\n",
    "X = torch.linspace(0,1,5)\n",
    "Z = torch.ones(len(X))\n",
    "coord = torch.stack([X,X,Z],dim=1)\n",
    "coord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "46. 在直角坐标轴上 (0,0) 到 (1,1) 区域内放一个 5x5 的网格，创建一个5x5x2 的 Tensor，其值对应网格中每个点的坐标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000],\n",
       "         [0.0000, 0.2500],\n",
       "         [0.0000, 0.5000],\n",
       "         [0.0000, 0.7500],\n",
       "         [0.0000, 1.0000]],\n",
       "\n",
       "        [[0.2500, 0.0000],\n",
       "         [0.2500, 0.2500],\n",
       "         [0.2500, 0.5000],\n",
       "         [0.2500, 0.7500],\n",
       "         [0.2500, 1.0000]],\n",
       "\n",
       "        [[0.5000, 0.0000],\n",
       "         [0.5000, 0.2500],\n",
       "         [0.5000, 0.5000],\n",
       "         [0.5000, 0.7500],\n",
       "         [0.5000, 1.0000]],\n",
       "\n",
       "        [[0.7500, 0.0000],\n",
       "         [0.7500, 0.2500],\n",
       "         [0.7500, 0.5000],\n",
       "         [0.7500, 0.7500],\n",
       "         [0.7500, 1.0000]],\n",
       "\n",
       "        [[1.0000, 0.0000],\n",
       "         [1.0000, 0.2500],\n",
       "         [1.0000, 0.5000],\n",
       "         [1.0000, 0.7500],\n",
       "         [1.0000, 1.0000]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,Y = torch.meshgrid(torch.linspace(0,1,5),\n",
    "                     torch.linspace(0,1,5))\n",
    "torch.stack([X,Y],dim=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "47. 给定两个向量X,Y, 创建一个柯西矩阵C(Cij=1/(xi-yj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n",
      "tensor([[-3., -3., -3., -3., -3.],\n",
      "        [-2., -2., -2., -2., -2.],\n",
      "        [-1., -1., -1., -1., -1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3333, -0.3333, -0.3333, -0.3333, -0.3333],\n",
       "        [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
       "        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
       "        [    inf,     inf,     inf,     inf,     inf],\n",
       "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(5)\n",
    "Y = torch.ones(5)*3\n",
    "C = torch.ones([5,5])\n",
    "X.view([-1,1])\n",
    "print(X.view([-1,1]).shape)\n",
    "print(X.view([-1,1]) - Y)\n",
    "torch.div(1, X.view([-1,1]) - Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "48. 打印PyTorch Tensor每一种标量数据结构的最大和最小可表示数(representable value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-128\n",
      "127\n",
      "-2147483648\n",
      "2147483647\n",
      "-9223372036854775808\n",
      "9223372036854775807\n",
      "================\n",
      "-3.4028234663852886e+38\n",
      "3.4028234663852886e+38\n",
      "1.1920928955078125e-07\n",
      "-1.7976931348623157e+308\n",
      "1.7976931348623157e+308\n",
      "2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "for dtype in [torch.int8, torch.int32, torch.int64]:\n",
    "    print(torch.iinfo(dtype).min)\n",
    "    print(torch.iinfo(dtype).max)\n",
    "\n",
    "print(\"==\"*8)\n",
    "for dtype in [torch.float32, torch.float64]:\n",
    "    print(torch.finfo(dtype).min)\n",
    "    print(torch.finfo(dtype).max)\n",
    "    print(torch.finfo(dtype).eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "49. 如何完整显示 Tensor 的所有值? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# torch.set_printoptions(threshold=float('NaN'))\n",
    "Z = torch.zeros([16,16])\n",
    "print(Z)\n",
    "torch.set_printoptions(threshold=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50. 给定一个标量和一个向量，找出向量中与给定标量值最接近的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5690, 0.2068, 0.6635, 0.9942, 0.8943])\n",
      "tensor(0.2068)\n",
      "tensor(0.2068)\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(5)\n",
    "print(t)\n",
    "scalar = 0.08\n",
    "b = torch.square(t-scalar)\n",
    "print(t[torch.argmin(b)])\n",
    "i = torch.abs(t-scalar).argmin()\n",
    "print(t[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "52. 给定一个(100,2)的矩阵，每行表示一个点的坐标。计算所有点两两之间的距离。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pointA:tensor([98.4723, 42.2691]), pointB:tensor([34.9820, 93.2735])的距离\n",
      "torch.Size([1, 5])\n",
      "torch.Size([5, 5])\n",
      "tensor([[0.0000, 0.0131, 0.4974, 0.3261, 0.6493],\n",
      "        [0.0131, 0.0000, 0.5000, 0.3285, 0.6503],\n",
      "        [0.4974, 0.5000, 0.0000, 0.1715, 0.1661],\n",
      "        [0.3261, 0.3285, 0.1715, 0.0000, 0.3265],\n",
      "        [0.6493, 0.6503, 0.1661, 0.3265, 0.0000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.01310947, 0.49737071, 0.32607643, 0.64927238],\n",
       "       [0.01310947, 0.        , 0.49998494, 0.3285432 , 0.65031431],\n",
       "       [0.49737071, 0.49998494, 0.        , 0.17145231, 0.16611076],\n",
       "       [0.32607643, 0.3285432 , 0.17145231, 0.        , 0.32645763],\n",
       "       [0.64927238, 0.65031431, 0.16611076, 0.32645763, 0.        ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.rand(100,2)*100\n",
    "def dist_2points(point1, point2):\n",
    "    vec = point2 - point1\n",
    "    return torch.sqrt(torch.square(vec).sum())\n",
    "def dist_2rows(matrix, i, j):\n",
    "    vec1 = matrix[i,:]\n",
    "    vec2 = matrix[j,:]\n",
    "    return dist_2points(vec1, vec2)\n",
    "print(f\"pointA:{A[0,:]}, pointB:{A[5,:]}的距离\")\n",
    "dist_2rows(A, 0, 5)\n",
    "t = torch.rand([5,2])\n",
    "X = t[:,0].view([1,-1])\n",
    "Y = t[:,1].view([1,-1])\n",
    "print(X.shape)\n",
    "print((X-X.T).shape)\n",
    "print(torch.sqrt((X-X.T)**2 + (Y-Y.T)**2))\n",
    "\n",
    "# 第二种，使用scipy\n",
    "import scipy\n",
    "import scipy.spatial\n",
    "D = scipy.spatial.distance.cdist(t,t)\n",
    "D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "53. 原地将一个浮点型(32 bits) Tensor 转换为整型(32 bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15 53 33 20 75 36 22 58 84 24]\n",
      "torch.FloatTensor\n",
      "tensor([0, 1, 3], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "Z = (np.random.rand(10)*100).astype(np.float32)\n",
    "Y = Z.view(np.int32)\n",
    "Y[:]=Z\n",
    "print(Y)\n",
    "A = torch.tensor([0.0, 1.2, 3.6], dtype=torch.float32)\n",
    "print(A.type())\n",
    "B = A.type(torch.int16)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "54. 将一下内容的Tensor保存至硬盘，并再次读取\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3],\n",
      "        [ 8,  9,  7],\n",
      "        [10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "T = torch.tensor([[1,2,3],\n",
    "                  [8,9,7],\n",
    "                  [10, 11, 12]])\n",
    "torch.save(T,'tensor.pt')\n",
    "load_tensor = torch.load('tensor.pt')\n",
    "print(load_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "55. enumerate 函数在PyTorch中对应的函数是什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0) 0\n",
      "(0, 1, 0) 1\n",
      "(0, 2, 0) 2\n",
      "(1, 0, 0) 3\n",
      "(1, 1, 0) 4\n",
      "(1, 2, 0) 5\n",
      "(2, 0, 0) 6\n",
      "(2, 1, 0) 7\n",
      "(2, 2, 0) 8\n",
      "[[0]\n",
      " [1]\n",
      " [2]] 0\n",
      "[[3]\n",
      " [4]\n",
      " [5]] 1\n",
      "[[6]\n",
      " [7]\n",
      " [8]] 2\n"
     ]
    }
   ],
   "source": [
    "Z = np.arange(9).reshape(3,3,1)\n",
    "# ndenumerate 是每一个元素\n",
    "for index, value in np.ndenumerate(Z):\n",
    "    print(index, value)\n",
    "\n",
    "# enumerate 是每一行\n",
    "for index,value in enumerate(Z):\n",
    "    print(value, index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,\n",
       "          0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ],\n",
       "        [-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,\n",
       "          0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ],\n",
       "        [-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,\n",
       "          0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ],\n",
       "        [-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,\n",
       "          0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ],\n",
       "        [-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,\n",
       "          0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ],\n",
       "        [-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,\n",
       "          0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ],\n",
       "        [-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,\n",
       "          0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ],\n",
       "        [-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,\n",
       "          0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ],\n",
       "        [-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,\n",
       "          0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ],\n",
       "        [-1.        , -0.77777778, -0.55555556, -0.33333333, -0.11111111,\n",
       "          0.11111111,  0.33333333,  0.55555556,  0.77777778,  1.        ]]),\n",
       " array([[-1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "         -1.        , -1.        , -1.        , -1.        , -1.        ],\n",
       "        [-0.77777778, -0.77777778, -0.77777778, -0.77777778, -0.77777778,\n",
       "         -0.77777778, -0.77777778, -0.77777778, -0.77777778, -0.77777778],\n",
       "        [-0.55555556, -0.55555556, -0.55555556, -0.55555556, -0.55555556,\n",
       "         -0.55555556, -0.55555556, -0.55555556, -0.55555556, -0.55555556],\n",
       "        [-0.33333333, -0.33333333, -0.33333333, -0.33333333, -0.33333333,\n",
       "         -0.33333333, -0.33333333, -0.33333333, -0.33333333, -0.33333333],\n",
       "        [-0.11111111, -0.11111111, -0.11111111, -0.11111111, -0.11111111,\n",
       "         -0.11111111, -0.11111111, -0.11111111, -0.11111111, -0.11111111],\n",
       "        [ 0.11111111,  0.11111111,  0.11111111,  0.11111111,  0.11111111,\n",
       "          0.11111111,  0.11111111,  0.11111111,  0.11111111,  0.11111111],\n",
       "        [ 0.33333333,  0.33333333,  0.33333333,  0.33333333,  0.33333333,\n",
       "          0.33333333,  0.33333333,  0.33333333,  0.33333333,  0.33333333],\n",
       "        [ 0.55555556,  0.55555556,  0.55555556,  0.55555556,  0.55555556,\n",
       "          0.55555556,  0.55555556,  0.55555556,  0.55555556,  0.55555556],\n",
       "        [ 0.77777778,  0.77777778,  0.77777778,  0.77777778,  0.77777778,\n",
       "          0.77777778,  0.77777778,  0.77777778,  0.77777778,  0.77777778],\n",
       "        [ 1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "          1.        ,  1.        ,  1.        ,  1.        ,  1.        ]])]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.meshgrid(np.linspace(-1,1,10), np.linspace(-1,1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "57.一个矩阵中随机选p个元素，并替换成其他值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([[ 1.,  1.,  1.,  1., 10.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [10.,  1., 10.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000],\n",
       "        [0.3268, 0.8912, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0700]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.ones([5,5])\n",
    "a = torch.randint(0,5,[3])\n",
    "b = torch.randint(0,5,[3])\n",
    "print(A)\n",
    "A[a,b] = 10\n",
    "print(A)\n",
    "p = 3\n",
    "t = torch.zeros([3,3])\n",
    "indices = torch.from_numpy(np.random.choice(range(9),p))\n",
    "# accumulate在重复索引时，可以累加， 正常就是替换值\n",
    "t.put_(indices, torch.rand(p), accumulate=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "58. 给定一个矩阵，计算每个元素减去所在行的平均数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.26624596  0.31192893 -0.02594173 -0.11735249 -0.43488085]\n",
      " [-0.070508   -0.25434566  0.37285507  0.26254374 -0.31054515]\n",
      " [ 0.06071758 -0.52152383  0.2887388  -0.16201949  0.334087  ]\n",
      " [-0.39359826 -0.31351632  0.43084347  0.33103502 -0.05476415]\n",
      " [-0.097036   -0.27569538  0.3792827   0.17863762 -0.18518907]]\n",
      "tensor([[ 0.2662,  0.3119, -0.0259, -0.1174, -0.4349],\n",
      "        [-0.0705, -0.2543,  0.3729,  0.2625, -0.3105],\n",
      "        [ 0.0607, -0.5215,  0.2887, -0.1620,  0.3341],\n",
      "        [-0.3936, -0.3135,  0.4308,  0.3310, -0.0548],\n",
      "        [-0.0970, -0.2757,  0.3793,  0.1786, -0.1852]])\n",
      "nokeepdim, tensor([2., 7.])\n",
      "withkeepdim, tensor([[2.],\n",
      "        [7.]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand([5,5])\n",
    "A_np = A.numpy()\n",
    "### 注意keepdim=True\n",
    "# 计算列均值\n",
    "col_means = np.mean(A_np, axis=1, keepdims=True)  # 形状 (5,)\n",
    "\n",
    "# 每个元素减去对应行的均值\n",
    "B = A_np - col_means \n",
    "print(B)\n",
    "print(A - torch.mean(A,dim=1, keepdim=True))\n",
    "Y = A - A.mean(axis=1,keepdim=True)\n",
    "Y\n",
    "X = torch.arange(10).view([2,5]).type(torch.float32)\n",
    "print(\"nokeepdim,\" ,torch.mean(X,dim=1))\n",
    "print(\"withkeepdim,\", torch.mean(X,dim=1, keepdim=True))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "59. 给定一个矩阵，根据第n列的值对行进行排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.randint(0, 10, [5,2])\n",
    "n = 1\n",
    "_, indices = M[:,n].sort()\n",
    "M = M[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 0, 0],\n",
      "        [0, 2, 1, 0],\n",
      "        [0, 0, 2, 0]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "t = torch.randint(0,3,[3,4])\n",
    "t[:,-1]=0\n",
    "print(t)\n",
    "print((t==0).all(dim=0).any().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "62. 给一个3维数组，计算数组中元素两两之和，得到一个(3,3)的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9, 8, 3]])\n",
      "tensor([[6],\n",
      "        [9],\n",
      "        [2]])\n",
      "tensor([[15, 14,  9],\n",
      "        [18, 17, 12],\n",
      "        [11, 10,  5]])\n",
      "tensor([[0, 1, 2],\n",
      "        [1, 2, 3],\n",
      "        [2, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randint(0,10,(1,3))\n",
    "b = torch.randint(0,10, (3,1))\n",
    "print(a)\n",
    "print(b)\n",
    "c = a + b\n",
    "print(c)\n",
    "A = torch.arange(3).view((1,3))\n",
    "B = torch.arange(3).view((3,1))\n",
    "print(A+B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "63. 创建一个继承自Pytorch.Tensor的类，并赋予其name_属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'some_name'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class Test(torch.Tensor):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.name_ = \"shui\"\n",
    "\n",
    "# test = Test([1, 2, 3])  # Create a tensor with some data\n",
    "# print(test.name_)\n",
    "\n",
    "class NamedTensor(torch.Tensor):\n",
    "    @staticmethod\n",
    "    def __new__(cls, x, name, *args, **kwargs):\n",
    "        obj = super().__new__(cls, x, *args, **kwargs)\n",
    "        obj.name_ = name\n",
    "        return obj\n",
    "\n",
    "Z = NamedTensor(torch.rand([3,3]), \"some_name\")\n",
    "Z.name_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 4.])\n",
      "tensor([6, 6, 4, 7, 8, 4, 7, 8, 7, 4])\n",
      "tensor([0, 0, 0, 0, 3, 0, 2, 3, 2])\n",
      "tensor([0., 7., 0., 6., 5., 0., 0., 0., 0., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# A = torch.tensor([1,1,1])\n",
    "# B = torch.tensor([1,2,2,2])\n",
    "# for i in B:\n",
    "#     A[i] += 1 # source\n",
    "# print(A)\n",
    "A = torch.tensor([1, 1, 1], dtype=torch.float32)  # 需要是浮点数\n",
    "B = torch.tensor([1, 2, 2, 2])                     # 索引\n",
    "source = torch.ones(B.shape[0])  # \n",
    "A.index_add_(0, B, source) # A[index[j]] += source[j] * alpha\n",
    "print(A)\n",
    "\n",
    "x = torch.tensor([1, 2, 1, 5, 3, 2, 1])\n",
    "x = torch.randint(4,9,[10])\n",
    "print(x)\n",
    "result = torch.bincount(x)\n",
    "print(result)  # tensor([1, 3, 2, 1])\n",
    "result[3]\n",
    "\n",
    "X = torch.Tensor([1,2,3,4,5,6]).type(torch.int64)\n",
    "I = torch.Tensor([1,3,9,3,4,1]).type(torch.int64)\n",
    "F = torch.bincount(I,X)\n",
    "print(F)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "66. 给定一个(w,h,3)的图像，判断像素中有多少个不重复的点。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "手动计算: 6592200\n",
      "二进制: 0b11001001001011011001000\n",
      "位运算: 6592200\n",
      "是否相等: True\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "I = torch.randint(0,255,[10,10,3]).type(torch.int8)\n",
    "F = I[...,0]*256*256 + I[...,1]*256 + I[...,2]\n",
    "n = len(torch.unique(F))\n",
    "print(n)\n",
    "\n",
    "R = 100  # 二进制: 01100100\n",
    "G = 150  # 二进制: 10010110  \n",
    "B = 200  # 二进制: 11001000\n",
    "\n",
    "# 手动计算\n",
    "F_manual = 100 * 65536 + 150 * 256 + 200\n",
    "print(f\"手动计算: {F_manual}\")\n",
    "print(f\"二进制: {bin(F_manual)}\")\n",
    "\n",
    "# 验证\n",
    "F_calc = (100 << 16) | (150 << 8) | 200\n",
    "print(f\"位运算: {F_calc}\")\n",
    "print(f\"是否相等: {F_manual == F_calc}\")\n",
    "print(len(bin(255)))\n",
    "print(len(bin(256)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25, 38, 49, 33],\n",
       "        [43, 41, 50, 27],\n",
       "        [24, 41, 30, 45],\n",
       "        [51, 41, 33, 45]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randint(0,10,[4,4,3,3])\n",
    "A.sum(axis=(-1,-2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "68. 给定变量D以及另一个同长度向量S。计算S中所有值相同元素对应的同位置D中的元素的平均数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000,    nan, 0.6667, 0.0000], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分组平均值:\n",
      "S\n",
      "0    1.000000\n",
      "2    0.666667\n",
      "3    0.000000\n",
      "Name: D, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "D = torch.tensor([1,1,0,1,0])\n",
    "S = torch.tensor([2,0,3,2,2])\n",
    "a_sum = torch.bincount(S, weights=D)\n",
    "D_counts = torch.bincount(S)\n",
    "D_means = a_sum/D_counts\n",
    "print(D_means)\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'S': S.numpy(),  # 分组标签\n",
    "    'D': D.numpy()  # 数据\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "result = df.groupby('S')['D'].mean()\n",
    "print(\"分组平均值:\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([76, 34])\n",
      "tensor(110)\n",
      "tensor([76, 34])\n",
      "tensor([[ 4, 72,  0],\n",
      "        [ 0, 32,  2]])\n",
      "tensor([76, 34])\n",
      "tensor([76, 34])\n"
     ]
    }
   ],
   "source": [
    "A = torch.randint(0,10,[2,3])\n",
    "B = torch.randint(0,10,[3,2])\n",
    "C = A@B\n",
    "print(C.diag())\n",
    "print(torch.sum(C.diag()))\n",
    "print(torch.diagonal(torch.mm(A,B)))\n",
    "print(A*B.T)\n",
    "print(torch.sum(A*B.T, axis=1))\n",
    "print(torch.einsum(\"ij,ji->i\", A, B))  #结果[i] = Σ_j A[i,j] × B[j,i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70. 给定向量 [1, 2, 3, 4, 5]，所有元素之间插入 3 个 0，得到新向量 [1, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 0, 4, 0, 0, 0, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 2., 0., 0., 0., 3., 0., 0., 0., 4., 0., 0., 0., 5.])\n",
      "tensor([1., 0., 2., 0., 3., 0., 4., 0., 5.])\n"
     ]
    }
   ],
   "source": [
    "t = torch.Tensor([1,2,3,4,5])\n",
    "t0 = torch.zeros([len(t) + (len(t)-1)*3])\n",
    "t0[::4] = t\n",
    "print(t0)\n",
    "print(t0[::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.]],\n",
      "\n",
      "        [[2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.]],\n",
      "\n",
      "        [[2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.]],\n",
      "\n",
      "        [[2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.]],\n",
      "\n",
      "        [[2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.],\n",
      "         [2., 2., 2.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.]],\n",
       "\n",
       "        [[2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.]],\n",
       "\n",
       "        [[2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.]],\n",
       "\n",
       "        [[2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.]],\n",
       "\n",
       "        [[2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.ones((5,5,3))\n",
    "B = torch.ones((5,5))*2\n",
    "A * B[:,:,None]\n",
    "B_expanded = B.unsqueeze(-1)\n",
    "result1 = A * B_expanded\n",
    "print(result1)\n",
    "A * B.view(5,5,1)  # 一定要升维，而不是降维处理\n",
    "# A_sum = A.sum(dim=-1)\n",
    "# result = A_sum * B\n",
    "# print(result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# D = C@B\n",
    "# D\n",
    "# C = A[:,:,0]@B\n",
    "# C\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 11, 12, 13, 14],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [ 0,  1,  2,  3,  4],\n",
      "        [15, 16, 17, 18, 19],\n",
      "        [20, 21, 22, 23, 24]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(25).view(5,5)\n",
    "A[[0,2]] = A[[2,0]]\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "74. 给定向量C是bincount函数的运行结果，如何计算出向量A，使得torch.bincount(A)==C?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 4, 5])\n",
      "tensor([1, 1, 2, 3, 4, 4, 6])\n",
      "tensor([1, 1, 2, 2, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "C = torch.tensor([0,2,4,5])\n",
    "A = torch.tensor([], dtype=torch.long)\n",
    "for index,i in enumerate(C):\n",
    "    if i!=0:\n",
    "        ones = torch.ones(int(i), dtype=torch.long) * index\n",
    "        A = torch.cat([A, ones])\n",
    "print(torch.bincount(A))\n",
    "\n",
    "t = torch.Tensor([1,1,2,3,4,4,6]).type(torch.int64)\n",
    "C = torch.bincount(t)\n",
    "T = torch.repeat_interleave(torch.arange(len(C)),C)\n",
    "b = torch.repeat_interleave(torch.tensor([1,2,4,5]), torch.tensor([2,3,0,1]))\n",
    "print(T)\n",
    "print(b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "75. 计算一个一维Tensor每个滑动窗口(sliding window)的平均数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14.,\n",
      "        15., 16., 17., 18., 19., 20., 21., 22., 23.])\n",
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14.,\n",
      "        15., 16., 17., 18.])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(25).type(torch.int16)\n",
    "def moving_average(A, window):\n",
    "    n = len(A)\n",
    "    result = []\n",
    "    for i in range(n-window+1):\n",
    "        window_sum = A[i:i+window].sum()\n",
    "        # print(A[i:i+window])\n",
    "        # print(window_sum/window)\n",
    "        result.append(window_sum/window)\n",
    "    return torch.tensor(result, dtype=torch.float32)\n",
    "print(moving_average(A,3))\n",
    "\n",
    "def moving_average(a, n=3):\n",
    "    ret = torch.cumsum(a, dim=0)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n-1:] /n\n",
    "Z = torch.arange(20)\n",
    "print(moving_average(Z, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "76. 给定一个一维 Tensor Z，创建一个二维 Tensor，其第一行是 (Z[0],Z[1],Z[2])，其后的每一行是上一行再后延 1 位（第二行就是 (Z[1],Z[2],Z[3])，最后一行就是 (Z[-3],Z[-2],Z[-1])）(★★★)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [1 2 3]\n",
      " [2 3 4]\n",
      " [3 4 5]\n",
      " [4 5 6]\n",
      " [5 6 7]\n",
      " [6 7 8]\n",
      " [7 8 9]]\n",
      "[[  0   1   2]\n",
      " [  1   2   3]\n",
      " [  2   3   4]\n",
      " [  3   4 100]\n",
      " [  4 100   6]\n",
      " [100   6   7]\n",
      " [  6   7   8]\n",
      " [  7   8   9]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.lib import stride_tricks\n",
    "\n",
    "def rolling(a, window):\n",
    "    shape = (a.size - window +1, window)\n",
    "    strides = (a.itemsize, a.itemsize)\n",
    "    return stride_tricks.as_strided(a, shape=shape,strides=strides)\n",
    "# as_strided创建了一个视图（不是副本）：\n",
    "a = np.arange(10)\n",
    "Z = rolling(a, 3)\n",
    "print(Z)\n",
    "a[5] = 100\n",
    "print(Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True,  True,  True, False])\n",
      "tensor([ True, False, False, False,  True])\n",
      "tensor([False,  True,  True,  True, False])\n",
      "tensor([ 0.0090,  0.8050, -1.2733, -1.4510,  1.0682])\n",
      "tensor([-0.0090, -0.8050,  1.2733,  1.4510, -1.0682])\n",
      "tensor([ 0.0090,  0.8050, -1.2733, -1.4510,  1.0682])\n"
     ]
    }
   ],
   "source": [
    "A = torch.randint(0,10, [5])\n",
    "B = A>5\n",
    "print(B)\n",
    "B[:] = ~B[:]\n",
    "print(B)\n",
    "torch.logical_not(B,out=B)\n",
    "print(B)\n",
    "D = torch.randn([5])\n",
    "print(D)\n",
    "D = D*-1\n",
    "print(D)\n",
    "torch.neg(D, out=D)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "78. P0 和 P1 是两个矩阵，每个矩阵每一行对应一个点的坐标。给定点 p，计算 p 到每一条 (P0[i],P1[i]) 组成的线的距离(★★★)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13.4386, 12.0983, 12.7036,  6.4963,  0.7720,  4.6535,  0.2261, 12.9893,\n",
      "         1.4731,  0.7220])\n",
      "tensor([[ 2.1077, 12.6936,  4.4553,  3.1830,  7.7769, 12.2475, 13.0769,  2.4084,\n",
      "          4.3368, 11.3206],\n",
      "        [ 5.5393,  0.1240,  1.1825,  8.5156,  4.8425, 12.8867,  2.6813,  5.8668,\n",
      "          8.4537,  0.4149],\n",
      "        [ 2.5433,  0.5660,  4.9238,  7.2023,  5.8930,  8.9155,  0.0591,  2.1159,\n",
      "          8.5305,  2.3444],\n",
      "        [ 8.0843, 12.0552,  5.0260,  4.6940,  3.6482,  3.0636,  5.5755,  5.8868,\n",
      "          2.4581,  5.2578],\n",
      "        [ 6.2406,  4.2844, 12.6775,  1.5845,  5.7335,  0.7170,  4.0108,  7.5866,\n",
      "          6.0061,  4.4741],\n",
      "        [ 1.2704,  1.8482, 13.6378,  7.7021, 11.2114,  1.3893,  7.7250,  4.2499,\n",
      "         12.0072,  9.1355],\n",
      "        [ 5.3939,  1.9886,  1.0825,  7.1073,  2.4286, 14.2239,  5.3069,  6.6699,\n",
      "          6.3654,  2.1753],\n",
      "        [ 2.5993,  5.4407,  5.8822,  1.9453,  2.1919,  5.5154,  1.7057,  2.1225,\n",
      "          3.9905,  0.3371],\n",
      "        [ 8.7901,  0.6125,  4.1621,  9.2344,  2.4079, 17.9993,  6.9725, 10.4542,\n",
      "          7.2715,  3.0885],\n",
      "        [10.4669,  9.8357, 11.4140,  3.8629,  0.6254,  2.2286,  0.3497, 10.2439,\n",
      "          0.5212,  0.0204]])\n"
     ]
    }
   ],
   "source": [
    "P0 = torch.arange(10).view([5,2])\n",
    "P1 = torch.randint(10,[5,2])\n",
    "def compute_dist(P0, P1, p):\n",
    "    T = P1 - P0\n",
    "    L = (T**2).sum(axis=1)\n",
    "    U = -((P0[:,0]-p[...,0])*T[:,0] + (P0[:,1]-p[...,1])*T[:,1])/ L\n",
    "    U = U.reshape([len(U),1])\n",
    "    D = P0 +U*T -p\n",
    "    return torch.sqrt((D**2).sum(axis=1))\n",
    "\n",
    "P0 = (torch.rand([10,2]) - 0.5)*20\n",
    "P1 = (torch.rand([10,2]) - 0.5)*20\n",
    "p = (torch.rand([1,2]) - 0.5)*20\n",
    "print(compute_dist(P0, P1, p))\n",
    "\n",
    "p = (torch.rand([10,2]) - 0.5)*20\n",
    "print(torch.stack([compute_dist(P0, P1, p_i) for p_i in p]))\n",
    "\n",
    "def compute_dist_3d(P0, P1, p):\n",
    "    T = P1 - P0\n",
    "    w = p - P0\n",
    "\n",
    "    dot_w_T = torch.sum(w*T, dim=1)\n",
    "    norm_T_sq = torch.sum(T**2, dim=1)\n",
    "    t = dot_w_T / norm_T_sq\n",
    "\n",
    "    proj_point = P0 + t.unsqueeze(1) *T\n",
    "\n",
    "    v = p-proj_point\n",
    "\n",
    "    distance = torch.sqrt(torch.sum(v**2, dim=1))\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20])\n",
      "tensor([20])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3]).view([1,3])\n",
    "b = torch.tensor([2,3,4]).view([1,3])\n",
    "dot = torch.sum(a*b,dim=1)\n",
    "print((a@b.T).squeeze(1))\n",
    "print(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 2  3  4  5]\n",
      " [ 3  4  5  6]\n",
      " [ 4  5  6  7]\n",
      " [ 5  6  7  8]\n",
      " [ 6  7  8  9]\n",
      " [ 7  8  9 10]\n",
      " [ 8  9 10 11]\n",
      " [ 9 10 11 12]\n",
      " [10 11 12 13]\n",
      " [11 12 13 14]]\n",
      "4\n",
      "[[ 1  2  3  4]\n",
      " [ 2  3  4  5]\n",
      " [ 3  4  5  6]\n",
      " [ 4  5  6  7]\n",
      " [ 5  6  7  8]\n",
      " [ 6  7  8  9]\n",
      " [ 7  8  9 10]\n",
      " [ 8  9 10 11]\n",
      " [ 9 10 11 12]\n",
      " [10 11 12 13]\n",
      " [11 12 13 14]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.lib import stride_tricks\n",
    "def rolling(a,window):\n",
    "    shape = (a.size - window +1, window)\n",
    "    strides = (a.itemsize, a.itemsize)\n",
    "    return stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "Z = rolling(np.arange(14)+1, 4)\n",
    "print(Z)\n",
    "\n",
    "Z = np.arange(1,15, dtype=np.uint32)\n",
    "print(Z.itemsize)\n",
    "R = stride_tricks.as_strided(Z, (11,4), (4,4))\n",
    "print(R)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "82. 计算一个矩阵的秩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10)\n",
      "10\n",
      "tensor(10)\n",
      "tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1])\n",
      "tensor(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.mode(\n",
       "values=tensor(0),\n",
       "indices=tensor(8))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1,1,1],[0,1,0],[1,1,1]],dtype=torch.float)\n",
    "A = torch.rand((10,10))\n",
    "print(torch.matrix_rank(A))\n",
    "S = torch.linalg.svdvals(A)\n",
    "tol = 1e-22\n",
    "if tol is None:\n",
    "    eps = torch.finfo(S.dtype).eps\n",
    "    tol = S.max()*max(A.shape)*eps\n",
    "rank = torch.sum(S>tol).item()\n",
    "print(rank)\n",
    "\n",
    "U, S, V = torch.svd(A)\n",
    "rank = torch.sum(S>1e-10)\n",
    "print(rank)\n",
    "\n",
    "a = torch.randint(0,2, [10])\n",
    "print(a)\n",
    "b = torch.argmax(torch.bincount(a))\n",
    "print(b)\n",
    "torch.mode(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "84. 从一个10x10的矩阵中，提取出所有的连续3x3小矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.3922, 1.6347, 6.5040],\n",
      "         [7.5139, 2.6396, 5.6477],\n",
      "         [3.2308, 5.0328, 9.9290]],\n",
      "\n",
      "        [[1.6347, 6.5040, 9.2954],\n",
      "         [2.6396, 5.6477, 0.3469],\n",
      "         [5.0328, 9.9290, 5.9292]],\n",
      "\n",
      "        [[7.5139, 2.6396, 5.6477],\n",
      "         [3.2308, 5.0328, 9.9290],\n",
      "         [5.2064, 4.1453, 9.2024]],\n",
      "\n",
      "        [[2.6396, 5.6477, 0.3469],\n",
      "         [5.0328, 9.9290, 5.9292],\n",
      "         [4.1453, 9.2024, 1.2758]]])\n",
      "tensor([[[[ 1.3922,  1.6347,  6.5040],\n",
      "          [ 7.5139,  2.6396,  5.6477],\n",
      "          [ 3.2308,  5.0328,  9.9290]],\n",
      "\n",
      "         [[ 1.6347,  6.5040, 50.0000],\n",
      "          [ 2.6396,  5.6477,  0.3469],\n",
      "          [ 5.0328,  9.9290,  5.9292]]],\n",
      "\n",
      "\n",
      "        [[[ 7.5139,  2.6396,  5.6477],\n",
      "          [ 3.2308,  5.0328,  9.9290],\n",
      "          [ 5.2064,  4.1453,  9.2024]],\n",
      "\n",
      "         [[ 2.6396,  5.6477,  0.3469],\n",
      "          [ 5.0328,  9.9290,  5.9292],\n",
      "          [ 4.1453,  9.2024,  1.2758]]]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand([4,4])*10\n",
    "b = torch.stack([A[i:i+3, j:j+3] for i in range(A.shape[0]-2) for j in range(A.shape[1]-2) ])\n",
    "\n",
    "# print(b)\n",
    "b.shape\n",
    "\n",
    "\"\"\"更高效\"\"\"\n",
    "# 原始Z: m×n矩阵\n",
    "# 内存布局: 行主序\n",
    "# Z.stride(): (n, 1)  # 行跨步=n，列跨步=1\n",
    "Z = A\n",
    "i = 1 + (Z.shape[0]-3)\n",
    "j = 1 + (Z.shape[1] -3)\n",
    "C = torch.as_strided(Z,size=[i,j,3,3], stride=Z.stride() + Z.stride())\n",
    "# print(C)\n",
    "A[0,3] = 50\n",
    "print(b)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "85. 创建一个 class，输入一个矩阵，将其转换为对称矩阵，同时要确保转换后的对称矩阵任意一个元素修改值后，对称位置元素也要修改 (★★★)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  5, 10, 15],\n",
      "        [ 5, 10, 15, 20],\n",
      "        [10, 15, 20, 25],\n",
      "        [15, 20, 25, 30]])\n",
      "tensor([[ 0,  5, 10, 15],\n",
      "        [ 5, 10, 15, 20],\n",
      "        [10, 15, 20, 42],\n",
      "        [15, 20, 42, 30]])\n"
     ]
    }
   ],
   "source": [
    "class Symetric:\n",
    "    def __init__(self, x:torch.tensor):\n",
    "        self.tensor = x + x.T\n",
    "    \n",
    "    def __setitem__(self, index, value):\n",
    "        i,j = index\n",
    "        self.tensor.__setitem__((i,j), value)\n",
    "        self.tensor.__setitem__((j,i), value)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.tensor.__repr__()\n",
    "\n",
    "t = Symetric(torch.arange(16).view(4,4))\n",
    "print(t)\n",
    "t[2,3] = 42\n",
    "print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16. 16. 16. 16.]\n",
      " [16. 16. 16. 16.]\n",
      " [16. 16. 16. 16.]\n",
      " [16. 16. 16. 16.]]\n"
     ]
    }
   ],
   "source": [
    "Z = np.ones((16,16))\n",
    "k = 4\n",
    "S = np.add.reduceat(np.add.reduceat(Z, np.arange(0, Z.shape[0], k), axis=0),\n",
    "                    np.arange(0, Z.shape[1], k), axis=1)\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "88.使用Tensor实现生命游戏(Game of Life)(★★★)\n",
    "译者注：生命游戏见 https://zh.wikipedia.org/wiki/%E5%BA%B7%E5%A8%81%E7%94%9F%E5%91%BD%E6%B8%B8%E6%88%8F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]\n",
      " [1 0 0 1 1]\n",
      " [0 1 1 1 0]]\n",
      "[[2 3 1]\n",
      " [3 4 4]\n",
      " [3 4 3]]\n",
      "[[1 1 1]\n",
      " [0 0 0]\n",
      " [0 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 1]\n",
      " [3 5 2]\n",
      " [1 3 0]]\n",
      "[[1 1 0]\n",
      " [1 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 3 2]\n",
      " [3 5 2]\n",
      " [2 2 2]]\n",
      "[[1 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0 2 0]\n",
      " [2 4 2]\n",
      " [0 2 0]]\n",
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 2 2]\n",
      " [2 4 2]\n",
      " [2 2 2]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 0 0 0 0]]\n",
      "[[2 4 2]\n",
      " [4 8 4]\n",
      " [2 4 2]]\n",
      "[[1 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def iterate(Z):\n",
    "    # Count neighbours\n",
    "    N = (Z[0:-2, 0:-2] + Z[0:-2, 1:-1] + Z[0:-2, 2:] +\n",
    "         Z[1:-1, 0:-2]                 + Z[1:-1, 2:] + \n",
    "         Z[2:  , 0:-2] + Z[2:  , 1:-1] + Z[2:  , 2:])\n",
    "    print(N)\n",
    "    print(Z[1:-1, 1:-1] )\n",
    "    # Apply rules\n",
    "    birth = (N==3) & (Z[1:-1, 1:-1] == 0)\n",
    "    survive = (N==2) | (N==3) & (Z[1:-1, 1:-1] == 1)\n",
    "    Z[...] = 0\n",
    "    Z[1:-1, 1:-1][birth | survive] = 1\n",
    "    return Z\n",
    "\n",
    "Z = np.random.randint(0,2,(5,5))\n",
    "print(Z)\n",
    "for i in range(100):\n",
    "    Z = iterate(Z)\n",
    "    print(Z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IsaacGym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
